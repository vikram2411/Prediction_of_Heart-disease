
I. Importing essential libraries
In [1]:

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline

import os
print(os.listdir())

import warnings
warnings.filterwarnings('ignore')

['.ipynb_checkpoints', 'heart.csv', 'Heart_disease_prediction.ipynb', 'README.md']

II. Importing and understanding our dataset
In [2]:

dataset = pd.read_csv("heart.csv")

Verifying it as a 'dataframe' object in pandas
In [3]:

type(dataset)

Out[3]:

pandas.core.frame.DataFrame

Shape of dataset
In [4]:

dataset.shape

Out[4]:

(303, 14)

Printing out a few columns
In [5]:

dataset.head(5)

Out[5]:
	age 	sex 	cp 	trestbps 	chol 	fbs 	restecg 	thalach 	exang 	oldpeak 	slope 	ca 	thal 	target
0 	63 	1 	3 	145 	233 	1 	0 	150 	0 	2.3 	0 	0 	1 	1
1 	37 	1 	2 	130 	250 	0 	1 	187 	0 	3.5 	0 	0 	2 	1
2 	41 	0 	1 	130 	204 	0 	0 	172 	0 	1.4 	2 	0 	2 	1
3 	56 	1 	1 	120 	236 	0 	1 	178 	0 	0.8 	2 	0 	2 	1
4 	57 	0 	0 	120 	354 	0 	1 	163 	1 	0.6 	2 	0 	2 	1
In [6]:

dataset.sample(5)

Out[6]:
	age 	sex 	cp 	trestbps 	chol 	fbs 	restecg 	thalach 	exang 	oldpeak 	slope 	ca 	thal 	target
248 	54 	1 	1 	192 	283 	0 	0 	195 	0 	0.0 	2 	1 	3 	0
147 	60 	0 	3 	150 	240 	0 	1 	171 	0 	0.9 	2 	0 	2 	1
239 	35 	1 	0 	126 	282 	0 	0 	156 	1 	0.0 	2 	0 	3 	0
4 	57 	0 	0 	120 	354 	0 	1 	163 	1 	0.6 	2 	0 	2 	1
7 	44 	1 	1 	120 	263 	0 	1 	173 	0 	0.0 	2 	0 	3 	1
Description
In [7]:

dataset.describe()

Out[7]:
	age 	sex 	cp 	trestbps 	chol 	fbs 	restecg 	thalach 	exang 	oldpeak 	slope 	ca 	thal 	target
count 	303.000000 	303.000000 	303.000000 	303.000000 	303.000000 	303.000000 	303.000000 	303.000000 	303.000000 	303.000000 	303.000000 	303.000000 	303.000000 	303.000000
mean 	54.366337 	0.683168 	0.966997 	131.623762 	246.264026 	0.148515 	0.528053 	149.646865 	0.326733 	1.039604 	1.399340 	0.729373 	2.313531 	0.544554
std 	9.082101 	0.466011 	1.032052 	17.538143 	51.830751 	0.356198 	0.525860 	22.905161 	0.469794 	1.161075 	0.616226 	1.022606 	0.612277 	0.498835
min 	29.000000 	0.000000 	0.000000 	94.000000 	126.000000 	0.000000 	0.000000 	71.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000
25% 	47.500000 	0.000000 	0.000000 	120.000000 	211.000000 	0.000000 	0.000000 	133.500000 	0.000000 	0.000000 	1.000000 	0.000000 	2.000000 	0.000000
50% 	55.000000 	1.000000 	1.000000 	130.000000 	240.000000 	0.000000 	1.000000 	153.000000 	0.000000 	0.800000 	1.000000 	0.000000 	2.000000 	1.000000
75% 	61.000000 	1.000000 	2.000000 	140.000000 	274.500000 	0.000000 	1.000000 	166.000000 	1.000000 	1.600000 	2.000000 	1.000000 	3.000000 	1.000000
max 	77.000000 	1.000000 	3.000000 	200.000000 	564.000000 	1.000000 	2.000000 	202.000000 	1.000000 	6.200000 	2.000000 	4.000000 	3.000000 	1.000000
In [8]:

dataset.info()

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 303 entries, 0 to 302
Data columns (total 14 columns):
age         303 non-null int64
sex         303 non-null int64
cp          303 non-null int64
trestbps    303 non-null int64
chol        303 non-null int64
fbs         303 non-null int64
restecg     303 non-null int64
thalach     303 non-null int64
exang       303 non-null int64
oldpeak     303 non-null float64
slope       303 non-null int64
ca          303 non-null int64
thal        303 non-null int64
target      303 non-null int64
dtypes: float64(1), int64(13)
memory usage: 33.2 KB

In [9]:

###Luckily, we have no missing values

Let's understand our columns better:
In [10]:

info = ["age","1: male, 0: female","chest pain type, 1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic","resting blood pressure"," serum cholestoral in mg/dl","fasting blood sugar > 120 mg/dl","resting electrocardiographic results (values 0,1,2)"," maximum heart rate achieved","exercise induced angina","oldpeak = ST depression induced by exercise relative to rest","the slope of the peak exercise ST segment","number of major vessels (0-3) colored by flourosopy","thal: 3 = normal; 6 = fixed defect; 7 = reversable defect"]



for i in range(len(info)):
    print(dataset.columns[i]+":\t\t\t"+info[i])

age:			age
sex:			1: male, 0: female
cp:			chest pain type, 1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic
trestbps:			resting blood pressure
chol:			 serum cholestoral in mg/dl
fbs:			fasting blood sugar > 120 mg/dl
restecg:			resting electrocardiographic results (values 0,1,2)
thalach:			 maximum heart rate achieved
exang:			exercise induced angina
oldpeak:			oldpeak = ST depression induced by exercise relative to rest
slope:			the slope of the peak exercise ST segment
ca:			number of major vessels (0-3) colored by flourosopy
thal:			thal: 3 = normal; 6 = fixed defect; 7 = reversable defect

Analysing the 'target' variable
In [11]:

dataset["target"].describe()

Out[11]:

count    303.000000
mean       0.544554
std        0.498835
min        0.000000
25%        0.000000
50%        1.000000
75%        1.000000
max        1.000000
Name: target, dtype: float64

In [12]:

dataset["target"].unique()

Out[12]:

array([1, 0])

Clearly, this is a classification problem, with the target variable having values '0' and '1'
Checking correlation between columns
In [13]:

print(dataset.corr()["target"].abs().sort_values(ascending=False))

target      1.000000
exang       0.436757
cp          0.433798
oldpeak     0.430696
thalach     0.421741
ca          0.391724
slope       0.345877
thal        0.344029
sex         0.280937
age         0.225439
trestbps    0.144931
restecg     0.137230
chol        0.085239
fbs         0.028046
Name: target, dtype: float64

In [14]:

#This shows that most columns are moderately correlated with target, but 'fbs' is very weakly correlated.

Exploratory Data Analysis (EDA)
First, analysing the target variable:
In [15]:

y = dataset["target"]

sns.countplot(y)


target_temp = dataset.target.value_counts()

print(target_temp)

1    165
0    138
Name: target, dtype: int64

In [16]:

print("Percentage of patience without heart problems: "+str(round(target_temp[0]*100/303,2)))
print("Percentage of patience with heart problems: "+str(round(target_temp[1]*100/303,2)))

#Alternatively,
# print("Percentage of patience with heart problems: "+str(y.where(y==1).count()*100/303))
# print("Percentage of patience with heart problems: "+str(y.where(y==0).count()*100/303))

# #Or,
# countNoDisease = len(df[df.target == 0])
# countHaveDisease = len(df[df.target == 1])

Percentage of patience without heart problems: 45.54
Percentage of patience with heart problems: 54.46

We'll analyse 'sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca' and 'thal' features
Analysing the 'Sex' feature
In [17]:

dataset["sex"].unique()

Out[17]:

array([1, 0])

We notice, that as expected, the 'sex' feature has 2 unique features
In [18]:

sns.barplot(dataset["sex"],y)

Out[18]:

<matplotlib.axes._subplots.AxesSubplot at 0x7f754be2e128>

We notice, that females are more likely to have heart problems than males
Analysing the 'Chest Pain Type' feature
In [19]:

dataset["cp"].unique()

Out[19]:

array([3, 2, 1, 0])

As expected, the CP feature has values from 0 to 3
In [20]:

sns.barplot(dataset["cp"],y)

Out[20]:

<matplotlib.axes._subplots.AxesSubplot at 0x7f754be17d30>

We notice, that chest pain of '0', i.e. the ones with typical angina are much less likely to have heart problems
Analysing the FBS feature
In [21]:

dataset["fbs"].describe()

Out[21]:

count    303.000000
mean       0.148515
std        0.356198
min        0.000000
25%        0.000000
50%        0.000000
75%        0.000000
max        1.000000
Name: fbs, dtype: float64

In [22]:

dataset["fbs"].unique()

Out[22]:

array([1, 0])

In [23]:

sns.barplot(dataset["fbs"],y)

Out[23]:

<matplotlib.axes._subplots.AxesSubplot at 0x7f754bddda90>

Nothing extraordinary here
Analysing the restecg feature
In [24]:

dataset["restecg"].unique()

Out[24]:

array([0, 1, 2])

In [25]:

sns.barplot(dataset["restecg"],y)

Out[25]:

<matplotlib.axes._subplots.AxesSubplot at 0x7f754bd3bdd8>

We realize that people with restecg '1' and '0' are much more likely to have a heart disease than with restecg '2'
Analysing the 'exang' feature
In [26]:

dataset["exang"].unique()

Out[26]:

array([0, 1])

In [27]:

sns.barplot(dataset["exang"],y)

Out[27]:

<matplotlib.axes._subplots.AxesSubplot at 0x7f754bc9eda0>

People with exang=1 i.e. Exercise induced angina are much less likely to have heart problems
Analysing the Slope feature
In [28]:

dataset["slope"].unique()

Out[28]:

array([0, 2, 1])

In [29]:

sns.barplot(dataset["slope"],y)

Out[29]:

<matplotlib.axes._subplots.AxesSubplot at 0x7f754bc75710>

We observe, that Slope '2' causes heart pain much more than Slope '0' and '1'
Analysing the 'ca' feature
In [30]:

#number of major vessels (0-3) colored by flourosopy

In [31]:

dataset["ca"].unique()

Out[31]:

array([0, 2, 1, 3, 4])

In [32]:

sns.countplot(dataset["ca"])

Out[32]:

<matplotlib.axes._subplots.AxesSubplot at 0x7f754bd13940>

In [33]:

sns.barplot(dataset["ca"],y)

Out[33]:

<matplotlib.axes._subplots.AxesSubplot at 0x7f754bc34c88>

ca=4 has astonishingly large number of heart patients
In [34]:

### Analysing the 'thal' feature

In [35]:

dataset["thal"].unique()

Out[35]:

array([1, 2, 3, 0])

In [36]:

sns.barplot(dataset["thal"],y)

Out[36]:

<matplotlib.axes._subplots.AxesSubplot at 0x7f754bb89128>

In [37]:

sns.distplot(dataset["thal"])

Out[37]:

<matplotlib.axes._subplots.AxesSubplot at 0x7f754baf1fd0>

IV. Train Test split
In [38]:

from sklearn.model_selection import train_test_split

predictors = dataset.drop("target",axis=1)
target = dataset["target"]

X_train,X_test,Y_train,Y_test = train_test_split(predictors,target,test_size=0.20,random_state=0)

In [39]:

X_train.shape

Out[39]:

(242, 13)

In [40]:

X_test.shape

Out[40]:

(61, 13)

In [41]:

Y_train.shape

Out[41]:

(242,)

In [42]:

Y_test.shape

Out[42]:

(61,)

V. Model Fitting
In [43]:

from sklearn.metrics import accuracy_score

Logistic Regression
In [44]:

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()

lr.fit(X_train,Y_train)

Y_pred_lr = lr.predict(X_test)

In [45]:

Y_pred_lr.shape

Out[45]:

(61,)

In [46]:

score_lr = round(accuracy_score(Y_pred_lr,Y_test)*100,2)

print("The accuracy score achieved using Logistic Regression is: "+str(score_lr)+" %")

The accuracy score achieved using Logistic Regression is: 85.25 %

Naive Bayes
In [47]:

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()

nb.fit(X_train,Y_train)

Y_pred_nb = nb.predict(X_test)

In [48]:

Y_pred_nb.shape

Out[48]:

(61,)

In [49]:

score_nb = round(accuracy_score(Y_pred_nb,Y_test)*100,2)

print("The accuracy score achieved using Naive Bayes is: "+str(score_nb)+" %")

The accuracy score achieved using Naive Bayes is: 85.25 %

SVM
In [50]:

from sklearn import svm

sv = svm.SVC(kernel='linear')

sv.fit(X_train, Y_train)

Y_pred_svm = sv.predict(X_test)

In [51]:

Y_pred_svm.shape

Out[51]:

(61,)

In [52]:

score_svm = round(accuracy_score(Y_pred_svm,Y_test)*100,2)

print("The accuracy score achieved using Linear SVM is: "+str(score_svm)+" %")

The accuracy score achieved using Linear SVM is: 81.97 %

K Nearest Neighbors
In [53]:

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train,Y_train)
Y_pred_knn=knn.predict(X_test)

In [54]:

Y_pred_knn.shape

Out[54]:

(61,)

In [55]:

score_knn = round(accuracy_score(Y_pred_knn,Y_test)*100,2)

print("The accuracy score achieved using KNN is: "+str(score_knn)+" %")

The accuracy score achieved using KNN is: 67.21 %

Decision Tree
In [56]:

from sklearn.tree import DecisionTreeClassifier

max_accuracy = 0


for x in range(200):
    dt = DecisionTreeClassifier(random_state=x)
    dt.fit(X_train,Y_train)
    Y_pred_dt = dt.predict(X_test)
    current_accuracy = round(accuracy_score(Y_pred_dt,Y_test)*100,2)
    if(current_accuracy>max_accuracy):
        max_accuracy = current_accuracy
        best_x = x
        
#print(max_accuracy)
#print(best_x)


dt = DecisionTreeClassifier(random_state=best_x)
dt.fit(X_train,Y_train)
Y_pred_dt = dt.predict(X_test)

In [57]:

print(Y_pred_dt.shape)

(61,)

In [58]:

score_dt = round(accuracy_score(Y_pred_dt,Y_test)*100,2)

print("The accuracy score achieved using Decision Tree is: "+str(score_dt)+" %")

The accuracy score achieved using Decision Tree is: 81.97 %

Random Forest
In [59]:

from sklearn.ensemble import RandomForestClassifier

max_accuracy = 0


for x in range(2000):
    rf = RandomForestClassifier(random_state=x)
    rf.fit(X_train,Y_train)
    Y_pred_rf = rf.predict(X_test)
    current_accuracy = round(accuracy_score(Y_pred_rf,Y_test)*100,2)
    if(current_accuracy>max_accuracy):
        max_accuracy = current_accuracy
        best_x = x
        
#print(max_accuracy)
#print(best_x)

rf = RandomForestClassifier(random_state=best_x)
rf.fit(X_train,Y_train)
Y_pred_rf = rf.predict(X_test)

In [60]:

Y_pred_rf.shape

Out[60]:

(61,)

In [61]:

score_rf = round(accuracy_score(Y_pred_rf,Y_test)*100,2)

print("The accuracy score achieved using Decision Tree is: "+str(score_rf)+" %")

The accuracy score achieved using Decision Tree is: 95.08 %

XGBoost
In [62]:

import xgboost as xgb

xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42)
xgb_model.fit(X_train, Y_train)

Y_pred_xgb = xgb_model.predict(X_test)

In [63]:

Y_pred_xgb.shape

Out[63]:

(61,)

In [64]:

score_xgb = round(accuracy_score(Y_pred_xgb,Y_test)*100,2)

print("The accuracy score achieved using XGBoost is: "+str(score_xgb)+" %")

The accuracy score achieved using XGBoost is: 85.25 %

Neural Network
In [65]:

from keras.models import Sequential
from keras.layers import Dense

Using TensorFlow backend.

In [66]:

# https://stats.stackexchange.com/a/136542 helped a lot in avoiding overfitting

model = Sequential()
model.add(Dense(11,activation='relu',input_dim=13))
model.add(Dense(1,activation='sigmoid'))

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

In [67]:

model.fit(X_train,Y_train,epochs=300)

Epoch 1/300
242/242 [==============================] - 3s 10ms/step - loss: 4.7817 - acc: 0.6074
Epoch 2/300
242/242 [==============================] - 0s 123us/step - loss: 2.6633 - acc: 0.6033
Epoch 3/300
242/242 [==============================] - 0s 124us/step - loss: 2.3371 - acc: 0.6281
Epoch 4/300
242/242 [==============================] - 0s 127us/step - loss: 2.2938 - acc: 0.6488
Epoch 5/300
242/242 [==============================] - 0s 127us/step - loss: 2.0712 - acc: 0.6860
Epoch 6/300
242/242 [==============================] - 0s 125us/step - loss: 2.0645 - acc: 0.6653
Epoch 7/300
242/242 [==============================] - 0s 124us/step - loss: 2.0023 - acc: 0.6860
Epoch 8/300
242/242 [==============================] - 0s 127us/step - loss: 1.9928 - acc: 0.6694
Epoch 9/300
242/242 [==============================] - 0s 129us/step - loss: 1.9670 - acc: 0.6694
Epoch 10/300
242/242 [==============================] - 0s 123us/step - loss: 1.9316 - acc: 0.6736
Epoch 11/300
242/242 [==============================] - 0s 123us/step - loss: 1.9223 - acc: 0.6901
Epoch 12/300
242/242 [==============================] - 0s 124us/step - loss: 1.8955 - acc: 0.6818
Epoch 13/300
242/242 [==============================] - 0s 124us/step - loss: 1.8573 - acc: 0.6901
Epoch 14/300
242/242 [==============================] - 0s 125us/step - loss: 1.8268 - acc: 0.6860
Epoch 15/300
242/242 [==============================] - 0s 122us/step - loss: 1.7915 - acc: 0.6860
Epoch 16/300
242/242 [==============================] - 0s 121us/step - loss: 1.7727 - acc: 0.6777
Epoch 17/300
242/242 [==============================] - 0s 122us/step - loss: 1.7275 - acc: 0.6860
Epoch 18/300
242/242 [==============================] - 0s 122us/step - loss: 1.7143 - acc: 0.7066
Epoch 19/300
242/242 [==============================] - 0s 124us/step - loss: 1.6761 - acc: 0.6860
Epoch 20/300
242/242 [==============================] - 0s 136us/step - loss: 1.6591 - acc: 0.6777
Epoch 21/300
242/242 [==============================] - 0s 128us/step - loss: 1.6486 - acc: 0.7107
Epoch 22/300
242/242 [==============================] - 0s 130us/step - loss: 1.6004 - acc: 0.6860
Epoch 23/300
242/242 [==============================] - 0s 125us/step - loss: 1.5765 - acc: 0.7107
Epoch 24/300
242/242 [==============================] - 0s 128us/step - loss: 1.5428 - acc: 0.7025
Epoch 25/300
242/242 [==============================] - 0s 125us/step - loss: 1.5179 - acc: 0.6901
Epoch 26/300
242/242 [==============================] - 0s 124us/step - loss: 1.5084 - acc: 0.7066
Epoch 27/300
242/242 [==============================] - 0s 124us/step - loss: 1.4619 - acc: 0.6942
Epoch 28/300
242/242 [==============================] - 0s 126us/step - loss: 1.4600 - acc: 0.6818
Epoch 29/300
242/242 [==============================] - 0s 123us/step - loss: 1.4072 - acc: 0.6942
Epoch 30/300
242/242 [==============================] - 0s 122us/step - loss: 1.4329 - acc: 0.6942
Epoch 31/300
242/242 [==============================] - 0s 123us/step - loss: 1.3622 - acc: 0.7066
Epoch 32/300
242/242 [==============================] - 0s 130us/step - loss: 1.3532 - acc: 0.7025
Epoch 33/300
242/242 [==============================] - 0s 127us/step - loss: 1.3160 - acc: 0.7066
Epoch 34/300
242/242 [==============================] - 0s 128us/step - loss: 1.3004 - acc: 0.7025
Epoch 35/300
242/242 [==============================] - 0s 123us/step - loss: 1.2741 - acc: 0.7025
Epoch 36/300
242/242 [==============================] - 0s 128us/step - loss: 1.2875 - acc: 0.7107
Epoch 37/300
242/242 [==============================] - 0s 124us/step - loss: 1.2634 - acc: 0.6983
Epoch 38/300
242/242 [==============================] - 0s 127us/step - loss: 1.2101 - acc: 0.6983
Epoch 39/300
242/242 [==============================] - 0s 123us/step - loss: 1.1919 - acc: 0.7025
Epoch 40/300
242/242 [==============================] - 0s 130us/step - loss: 1.2064 - acc: 0.6942
Epoch 41/300
242/242 [==============================] - 0s 123us/step - loss: 1.2389 - acc: 0.6818
Epoch 42/300
242/242 [==============================] - 0s 124us/step - loss: 1.1472 - acc: 0.7066
Epoch 43/300
242/242 [==============================] - 0s 125us/step - loss: 1.1238 - acc: 0.6942
Epoch 44/300
242/242 [==============================] - 0s 125us/step - loss: 1.1178 - acc: 0.7025
Epoch 45/300
242/242 [==============================] - 0s 126us/step - loss: 1.0960 - acc: 0.6983
Epoch 46/300
242/242 [==============================] - 0s 120us/step - loss: 1.0781 - acc: 0.7025
Epoch 47/300
242/242 [==============================] - 0s 123us/step - loss: 1.0553 - acc: 0.7025
Epoch 48/300
242/242 [==============================] - 0s 122us/step - loss: 1.0342 - acc: 0.7149
Epoch 49/300
242/242 [==============================] - 0s 123us/step - loss: 1.0223 - acc: 0.7066
Epoch 50/300
242/242 [==============================] - 0s 134us/step - loss: 1.0213 - acc: 0.7107
Epoch 51/300
242/242 [==============================] - 0s 128us/step - loss: 0.9977 - acc: 0.7231
Epoch 52/300
242/242 [==============================] - 0s 137us/step - loss: 0.9841 - acc: 0.7190
Epoch 53/300
242/242 [==============================] - 0s 127us/step - loss: 0.9594 - acc: 0.7149
Epoch 54/300
242/242 [==============================] - 0s 124us/step - loss: 0.9688 - acc: 0.7231
Epoch 55/300
242/242 [==============================] - 0s 127us/step - loss: 0.9942 - acc: 0.6901
Epoch 56/300
242/242 [==============================] - 0s 125us/step - loss: 0.9391 - acc: 0.7149
Epoch 57/300
242/242 [==============================] - 0s 130us/step - loss: 0.9229 - acc: 0.6942
Epoch 58/300
242/242 [==============================] - 0s 132us/step - loss: 0.8981 - acc: 0.7190
Epoch 59/300
242/242 [==============================] - 0s 134us/step - loss: 0.8885 - acc: 0.7190
Epoch 60/300
242/242 [==============================] - 0s 129us/step - loss: 0.8854 - acc: 0.7149
Epoch 61/300
242/242 [==============================] - 0s 123us/step - loss: 0.8573 - acc: 0.7066
Epoch 62/300
242/242 [==============================] - 0s 124us/step - loss: 0.8476 - acc: 0.7273
Epoch 63/300
242/242 [==============================] - 0s 123us/step - loss: 0.8410 - acc: 0.7149
Epoch 64/300
242/242 [==============================] - 0s 119us/step - loss: 0.8287 - acc: 0.7355
Epoch 65/300
242/242 [==============================] - 0s 129us/step - loss: 0.8199 - acc: 0.7397
Epoch 66/300
242/242 [==============================] - 0s 132us/step - loss: 0.8091 - acc: 0.7355
Epoch 67/300
242/242 [==============================] - 0s 121us/step - loss: 0.7976 - acc: 0.7107
Epoch 68/300
242/242 [==============================] - 0s 125us/step - loss: 0.7912 - acc: 0.7231
Epoch 69/300
242/242 [==============================] - 0s 132us/step - loss: 0.7775 - acc: 0.7479
Epoch 70/300
242/242 [==============================] - 0s 121us/step - loss: 0.7608 - acc: 0.7355
Epoch 71/300
242/242 [==============================] - 0s 125us/step - loss: 0.7553 - acc: 0.7231
Epoch 72/300
242/242 [==============================] - 0s 127us/step - loss: 0.7470 - acc: 0.7397
Epoch 73/300
242/242 [==============================] - 0s 129us/step - loss: 0.7429 - acc: 0.7521
Epoch 74/300
242/242 [==============================] - 0s 122us/step - loss: 0.7543 - acc: 0.7190
Epoch 75/300
242/242 [==============================] - 0s 120us/step - loss: 0.7206 - acc: 0.7438
Epoch 76/300
242/242 [==============================] - 0s 127us/step - loss: 0.7220 - acc: 0.7479
Epoch 77/300
242/242 [==============================] - 0s 125us/step - loss: 0.7237 - acc: 0.7066
Epoch 78/300
242/242 [==============================] - 0s 124us/step - loss: 0.7065 - acc: 0.7273
Epoch 79/300
242/242 [==============================] - 0s 127us/step - loss: 0.6893 - acc: 0.7314
Epoch 80/300
242/242 [==============================] - 0s 123us/step - loss: 0.6972 - acc: 0.7397
Epoch 81/300
242/242 [==============================] - 0s 122us/step - loss: 0.6626 - acc: 0.7273
Epoch 82/300
242/242 [==============================] - 0s 125us/step - loss: 0.6600 - acc: 0.7521
Epoch 83/300
242/242 [==============================] - 0s 122us/step - loss: 0.6686 - acc: 0.7231
Epoch 84/300
242/242 [==============================] - 0s 136us/step - loss: 0.6550 - acc: 0.7397
Epoch 85/300
242/242 [==============================] - 0s 137us/step - loss: 0.6363 - acc: 0.7397
Epoch 86/300
242/242 [==============================] - 0s 126us/step - loss: 0.6298 - acc: 0.7438
Epoch 87/300
242/242 [==============================] - 0s 130us/step - loss: 0.6332 - acc: 0.7562
Epoch 88/300
242/242 [==============================] - 0s 127us/step - loss: 0.6204 - acc: 0.7521
Epoch 89/300
242/242 [==============================] - 0s 128us/step - loss: 0.6155 - acc: 0.7438
Epoch 90/300
242/242 [==============================] - 0s 137us/step - loss: 0.6085 - acc: 0.7603
Epoch 91/300
242/242 [==============================] - 0s 132us/step - loss: 0.6035 - acc: 0.7273
Epoch 92/300
242/242 [==============================] - 0s 133us/step - loss: 0.5924 - acc: 0.7562
Epoch 93/300
242/242 [==============================] - 0s 124us/step - loss: 0.6020 - acc: 0.7562
Epoch 94/300
242/242 [==============================] - 0s 124us/step - loss: 0.5888 - acc: 0.7603
Epoch 95/300
242/242 [==============================] - 0s 125us/step - loss: 0.5771 - acc: 0.7355
Epoch 96/300
242/242 [==============================] - 0s 125us/step - loss: 0.5699 - acc: 0.7603
Epoch 97/300
242/242 [==============================] - 0s 123us/step - loss: 0.5732 - acc: 0.7521
Epoch 98/300
242/242 [==============================] - 0s 126us/step - loss: 0.5631 - acc: 0.7603
Epoch 99/300
242/242 [==============================] - 0s 129us/step - loss: 0.5576 - acc: 0.7645
Epoch 100/300
242/242 [==============================] - 0s 123us/step - loss: 0.5528 - acc: 0.7645
Epoch 101/300
242/242 [==============================] - 0s 128us/step - loss: 0.5459 - acc: 0.7521
Epoch 102/300
242/242 [==============================] - 0s 124us/step - loss: 0.5379 - acc: 0.7645
Epoch 103/300
242/242 [==============================] - 0s 127us/step - loss: 0.5654 - acc: 0.7810
Epoch 104/300
242/242 [==============================] - 0s 126us/step - loss: 0.5296 - acc: 0.7562
Epoch 105/300
242/242 [==============================] - 0s 127us/step - loss: 0.5355 - acc: 0.7769
Epoch 106/300
242/242 [==============================] - 0s 128us/step - loss: 0.5250 - acc: 0.7686
Epoch 107/300
242/242 [==============================] - 0s 127us/step - loss: 0.5180 - acc: 0.7769
Epoch 108/300
242/242 [==============================] - 0s 125us/step - loss: 0.5170 - acc: 0.7562
Epoch 109/300
242/242 [==============================] - 0s 126us/step - loss: 0.5071 - acc: 0.7810
Epoch 110/300
242/242 [==============================] - 0s 126us/step - loss: 0.5054 - acc: 0.7810
Epoch 111/300
242/242 [==============================] - 0s 130us/step - loss: 0.5196 - acc: 0.7769
Epoch 112/300
242/242 [==============================] - 0s 124us/step - loss: 0.5005 - acc: 0.7769
Epoch 113/300
242/242 [==============================] - 0s 124us/step - loss: 0.5047 - acc: 0.7727
Epoch 114/300
242/242 [==============================] - 0s 124us/step - loss: 0.4910 - acc: 0.7769
Epoch 115/300
242/242 [==============================] - 0s 121us/step - loss: 0.4900 - acc: 0.7769
Epoch 116/300
242/242 [==============================] - 0s 138us/step - loss: 0.5211 - acc: 0.7686
Epoch 117/300
242/242 [==============================] - 0s 127us/step - loss: 0.4756 - acc: 0.7893
Epoch 118/300
242/242 [==============================] - 0s 132us/step - loss: 0.4844 - acc: 0.7769
Epoch 119/300
242/242 [==============================] - 0s 124us/step - loss: 0.4720 - acc: 0.7934
Epoch 120/300
242/242 [==============================] - 0s 128us/step - loss: 0.4720 - acc: 0.7893
Epoch 121/300
242/242 [==============================] - 0s 127us/step - loss: 0.4669 - acc: 0.7893
Epoch 122/300
242/242 [==============================] - 0s 130us/step - loss: 0.4736 - acc: 0.7851
Epoch 123/300
242/242 [==============================] - 0s 130us/step - loss: 0.4580 - acc: 0.7975
Epoch 124/300
242/242 [==============================] - 0s 140us/step - loss: 0.4592 - acc: 0.7851
Epoch 125/300
242/242 [==============================] - 0s 124us/step - loss: 0.4571 - acc: 0.8017
Epoch 126/300
242/242 [==============================] - 0s 132us/step - loss: 0.4596 - acc: 0.7727
Epoch 127/300
242/242 [==============================] - 0s 130us/step - loss: 0.4581 - acc: 0.7893
Epoch 128/300
242/242 [==============================] - 0s 128us/step - loss: 0.4543 - acc: 0.7934
Epoch 129/300
242/242 [==============================] - 0s 130us/step - loss: 0.4893 - acc: 0.7727
Epoch 130/300
242/242 [==============================] - 0s 123us/step - loss: 0.4437 - acc: 0.8058
Epoch 131/300
242/242 [==============================] - 0s 125us/step - loss: 0.4630 - acc: 0.7975
Epoch 132/300
242/242 [==============================] - 0s 124us/step - loss: 0.4564 - acc: 0.7851
Epoch 133/300
242/242 [==============================] - 0s 125us/step - loss: 0.4346 - acc: 0.7975
Epoch 134/300
242/242 [==============================] - 0s 127us/step - loss: 0.4375 - acc: 0.7975
Epoch 135/300
242/242 [==============================] - 0s 125us/step - loss: 0.4382 - acc: 0.7975
Epoch 136/300
242/242 [==============================] - 0s 126us/step - loss: 0.4308 - acc: 0.8017
Epoch 137/300
242/242 [==============================] - 0s 123us/step - loss: 0.4332 - acc: 0.8017
Epoch 138/300
242/242 [==============================] - 0s 126us/step - loss: 0.4275 - acc: 0.8017
Epoch 139/300
242/242 [==============================] - 0s 126us/step - loss: 0.4311 - acc: 0.7975
Epoch 140/300
242/242 [==============================] - 0s 124us/step - loss: 0.4346 - acc: 0.8099
Epoch 141/300
242/242 [==============================] - 0s 126us/step - loss: 0.4289 - acc: 0.8223
Epoch 142/300
242/242 [==============================] - 0s 126us/step - loss: 0.4213 - acc: 0.8099
Epoch 143/300
242/242 [==============================] - 0s 126us/step - loss: 0.4319 - acc: 0.8058
Epoch 144/300
242/242 [==============================] - 0s 126us/step - loss: 0.4252 - acc: 0.8017
Epoch 145/300
242/242 [==============================] - 0s 127us/step - loss: 0.4389 - acc: 0.8182
Epoch 146/300
242/242 [==============================] - 0s 125us/step - loss: 0.4292 - acc: 0.8017
Epoch 147/300
242/242 [==============================] - 0s 138us/step - loss: 0.4251 - acc: 0.8058
Epoch 148/300
242/242 [==============================] - 0s 131us/step - loss: 0.4114 - acc: 0.8099
Epoch 149/300
242/242 [==============================] - 0s 136us/step - loss: 0.4233 - acc: 0.7975
Epoch 150/300
242/242 [==============================] - 0s 132us/step - loss: 0.4072 - acc: 0.8182
Epoch 151/300
242/242 [==============================] - 0s 131us/step - loss: 0.4449 - acc: 0.7851
Epoch 152/300
242/242 [==============================] - 0s 131us/step - loss: 0.4335 - acc: 0.7934
Epoch 153/300
242/242 [==============================] - 0s 134us/step - loss: 0.4077 - acc: 0.8182
Epoch 154/300
242/242 [==============================] - 0s 154us/step - loss: 0.4137 - acc: 0.8099
Epoch 155/300
242/242 [==============================] - 0s 139us/step - loss: 0.4079 - acc: 0.8182
Epoch 156/300
242/242 [==============================] - 0s 135us/step - loss: 0.4054 - acc: 0.8140
Epoch 157/300
242/242 [==============================] - 0s 131us/step - loss: 0.4072 - acc: 0.8223
Epoch 158/300
242/242 [==============================] - 0s 129us/step - loss: 0.4062 - acc: 0.8182
Epoch 159/300
242/242 [==============================] - 0s 134us/step - loss: 0.4031 - acc: 0.8306
Epoch 160/300
242/242 [==============================] - 0s 140us/step - loss: 0.4019 - acc: 0.8058
Epoch 161/300
242/242 [==============================] - 0s 130us/step - loss: 0.4005 - acc: 0.8223
Epoch 162/300
242/242 [==============================] - 0s 131us/step - loss: 0.4108 - acc: 0.8140
Epoch 163/300
242/242 [==============================] - 0s 136us/step - loss: 0.3946 - acc: 0.8264
Epoch 164/300
242/242 [==============================] - 0s 131us/step - loss: 0.3961 - acc: 0.8058
Epoch 165/300
242/242 [==============================] - 0s 143us/step - loss: 0.4076 - acc: 0.8223
Epoch 166/300
242/242 [==============================] - 0s 123us/step - loss: 0.4033 - acc: 0.8182
Epoch 167/300
242/242 [==============================] - 0s 128us/step - loss: 0.4171 - acc: 0.8140
Epoch 168/300
242/242 [==============================] - 0s 129us/step - loss: 0.3988 - acc: 0.8388
Epoch 169/300
242/242 [==============================] - 0s 125us/step - loss: 0.3987 - acc: 0.8182
Epoch 170/300
242/242 [==============================] - 0s 127us/step - loss: 0.4012 - acc: 0.8182
Epoch 171/300
242/242 [==============================] - 0s 137us/step - loss: 0.3952 - acc: 0.8140
Epoch 172/300
242/242 [==============================] - 0s 134us/step - loss: 0.4002 - acc: 0.8430
Epoch 173/300
242/242 [==============================] - 0s 125us/step - loss: 0.3978 - acc: 0.8099
Epoch 174/300
242/242 [==============================] - 0s 139us/step - loss: 0.4227 - acc: 0.8264
Epoch 175/300
242/242 [==============================] - 0s 126us/step - loss: 0.4119 - acc: 0.8347
Epoch 176/300
242/242 [==============================] - 0s 125us/step - loss: 0.4099 - acc: 0.8017
Epoch 177/300
242/242 [==============================] - 0s 132us/step - loss: 0.3934 - acc: 0.8388
Epoch 178/300
242/242 [==============================] - 0s 131us/step - loss: 0.3995 - acc: 0.8099
Epoch 179/300
242/242 [==============================] - 0s 134us/step - loss: 0.3905 - acc: 0.8264
Epoch 180/300
242/242 [==============================] - 0s 131us/step - loss: 0.3884 - acc: 0.8182
Epoch 181/300
242/242 [==============================] - 0s 135us/step - loss: 0.3856 - acc: 0.8471
Epoch 182/300
242/242 [==============================] - 0s 142us/step - loss: 0.3851 - acc: 0.8140
Epoch 183/300
242/242 [==============================] - 0s 156us/step - loss: 0.3884 - acc: 0.8430
Epoch 184/300
242/242 [==============================] - 0s 133us/step - loss: 0.3905 - acc: 0.8223
Epoch 185/300
242/242 [==============================] - 0s 141us/step - loss: 0.4023 - acc: 0.8140
Epoch 186/300
242/242 [==============================] - 0s 139us/step - loss: 0.3969 - acc: 0.8347
Epoch 187/300
242/242 [==============================] - 0s 128us/step - loss: 0.3801 - acc: 0.8264
Epoch 188/300
242/242 [==============================] - 0s 131us/step - loss: 0.4085 - acc: 0.7975
Epoch 189/300
242/242 [==============================] - 0s 135us/step - loss: 0.4002 - acc: 0.8099
Epoch 190/300
242/242 [==============================] - 0s 122us/step - loss: 0.3857 - acc: 0.8223
Epoch 191/300
242/242 [==============================] - 0s 131us/step - loss: 0.3961 - acc: 0.8471
Epoch 192/300
242/242 [==============================] - 0s 127us/step - loss: 0.3903 - acc: 0.8140
Epoch 193/300
242/242 [==============================] - 0s 126us/step - loss: 0.3921 - acc: 0.8306
Epoch 194/300
242/242 [==============================] - 0s 124us/step - loss: 0.4042 - acc: 0.8017
Epoch 195/300
242/242 [==============================] - 0s 125us/step - loss: 0.4186 - acc: 0.7934
Epoch 196/300
242/242 [==============================] - 0s 124us/step - loss: 0.3855 - acc: 0.8264
Epoch 197/300
242/242 [==============================] - 0s 129us/step - loss: 0.3961 - acc: 0.8140
Epoch 198/300
242/242 [==============================] - 0s 130us/step - loss: 0.3932 - acc: 0.8099
Epoch 199/300
242/242 [==============================] - 0s 129us/step - loss: 0.3860 - acc: 0.8264
Epoch 200/300
242/242 [==============================] - 0s 133us/step - loss: 0.3876 - acc: 0.8388
Epoch 201/300
242/242 [==============================] - 0s 126us/step - loss: 0.3915 - acc: 0.8347
Epoch 202/300
242/242 [==============================] - 0s 125us/step - loss: 0.4143 - acc: 0.8182
Epoch 203/300
242/242 [==============================] - 0s 122us/step - loss: 0.3690 - acc: 0.8223
Epoch 204/300
242/242 [==============================] - 0s 130us/step - loss: 0.3847 - acc: 0.8347
Epoch 205/300
242/242 [==============================] - 0s 126us/step - loss: 0.3764 - acc: 0.8306
Epoch 206/300
242/242 [==============================] - 0s 127us/step - loss: 0.3731 - acc: 0.8347
Epoch 207/300
242/242 [==============================] - 0s 126us/step - loss: 0.3760 - acc: 0.8430
Epoch 208/300
242/242 [==============================] - 0s 137us/step - loss: 0.3779 - acc: 0.8388
Epoch 209/300
242/242 [==============================] - 0s 128us/step - loss: 0.3776 - acc: 0.8223
Epoch 210/300
242/242 [==============================] - 0s 123us/step - loss: 0.3848 - acc: 0.8430
Epoch 211/300
242/242 [==============================] - 0s 124us/step - loss: 0.3753 - acc: 0.8471
Epoch 212/300
242/242 [==============================] - 0s 123us/step - loss: 0.3796 - acc: 0.8388
Epoch 213/300
242/242 [==============================] - 0s 130us/step - loss: 0.3736 - acc: 0.8347
Epoch 214/300
242/242 [==============================] - 0s 125us/step - loss: 0.3741 - acc: 0.8306
Epoch 215/300
242/242 [==============================] - 0s 125us/step - loss: 0.3748 - acc: 0.8306
Epoch 216/300
242/242 [==============================] - 0s 132us/step - loss: 0.3738 - acc: 0.8347
Epoch 217/300
242/242 [==============================] - 0s 122us/step - loss: 0.3754 - acc: 0.8471
Epoch 218/300
242/242 [==============================] - 0s 122us/step - loss: 0.3826 - acc: 0.8388
Epoch 219/300
242/242 [==============================] - 0s 127us/step - loss: 0.3900 - acc: 0.8264
Epoch 220/300
242/242 [==============================] - 0s 122us/step - loss: 0.3745 - acc: 0.8430
Epoch 221/300
242/242 [==============================] - 0s 129us/step - loss: 0.3828 - acc: 0.8264
Epoch 222/300
242/242 [==============================] - 0s 124us/step - loss: 0.3902 - acc: 0.8306
Epoch 223/300
242/242 [==============================] - 0s 123us/step - loss: 0.3700 - acc: 0.8512
Epoch 224/300
242/242 [==============================] - 0s 126us/step - loss: 0.3746 - acc: 0.8388
Epoch 225/300
242/242 [==============================] - 0s 126us/step - loss: 0.3863 - acc: 0.8430
Epoch 226/300
242/242 [==============================] - 0s 124us/step - loss: 0.3698 - acc: 0.8347
Epoch 227/300
242/242 [==============================] - 0s 131us/step - loss: 0.3811 - acc: 0.8388
Epoch 228/300
242/242 [==============================] - 0s 127us/step - loss: 0.3801 - acc: 0.8306
Epoch 229/300
242/242 [==============================] - 0s 125us/step - loss: 0.3742 - acc: 0.8347
Epoch 230/300
242/242 [==============================] - 0s 124us/step - loss: 0.3743 - acc: 0.8430
Epoch 231/300
242/242 [==============================] - 0s 125us/step - loss: 0.3958 - acc: 0.8264
Epoch 232/300
242/242 [==============================] - 0s 130us/step - loss: 0.3665 - acc: 0.8347
Epoch 233/300
242/242 [==============================] - 0s 143us/step - loss: 0.3952 - acc: 0.8347
Epoch 234/300
242/242 [==============================] - 0s 126us/step - loss: 0.3763 - acc: 0.8264
Epoch 235/300
242/242 [==============================] - 0s 125us/step - loss: 0.3685 - acc: 0.8512
Epoch 236/300
242/242 [==============================] - 0s 126us/step - loss: 0.3707 - acc: 0.8347
Epoch 237/300
242/242 [==============================] - 0s 125us/step - loss: 0.4014 - acc: 0.8017
Epoch 238/300
242/242 [==============================] - 0s 125us/step - loss: 0.4119 - acc: 0.8223
Epoch 239/300
242/242 [==============================] - 0s 126us/step - loss: 0.3824 - acc: 0.8388
Epoch 240/300
242/242 [==============================] - 0s 152us/step - loss: 0.3735 - acc: 0.8388
Epoch 241/300
242/242 [==============================] - 0s 128us/step - loss: 0.3757 - acc: 0.8347
Epoch 242/300
242/242 [==============================] - 0s 131us/step - loss: 0.3697 - acc: 0.8471
Epoch 243/300
242/242 [==============================] - 0s 129us/step - loss: 0.3718 - acc: 0.8223
Epoch 244/300
242/242 [==============================] - 0s 125us/step - loss: 0.3763 - acc: 0.8306
Epoch 245/300
242/242 [==============================] - 0s 127us/step - loss: 0.3705 - acc: 0.8306
Epoch 246/300
242/242 [==============================] - 0s 129us/step - loss: 0.3737 - acc: 0.8347
Epoch 247/300
242/242 [==============================] - 0s 125us/step - loss: 0.3676 - acc: 0.8388
Epoch 248/300
242/242 [==============================] - 0s 124us/step - loss: 0.3702 - acc: 0.8471
Epoch 249/300
242/242 [==============================] - 0s 126us/step - loss: 0.3711 - acc: 0.8471
Epoch 250/300
242/242 [==============================] - 0s 125us/step - loss: 0.3762 - acc: 0.8471
Epoch 251/300
242/242 [==============================] - 0s 125us/step - loss: 0.3684 - acc: 0.8471
Epoch 252/300
242/242 [==============================] - 0s 133us/step - loss: 0.3945 - acc: 0.8182
Epoch 253/300
242/242 [==============================] - 0s 128us/step - loss: 0.3693 - acc: 0.8430
Epoch 254/300
242/242 [==============================] - 0s 125us/step - loss: 0.3672 - acc: 0.8388
Epoch 255/300
242/242 [==============================] - 0s 129us/step - loss: 0.3718 - acc: 0.8512
Epoch 256/300
242/242 [==============================] - 0s 124us/step - loss: 0.3768 - acc: 0.8347
Epoch 257/300
242/242 [==============================] - 0s 127us/step - loss: 0.3852 - acc: 0.8388
Epoch 258/300
242/242 [==============================] - 0s 124us/step - loss: 0.3752 - acc: 0.8471
Epoch 259/300
242/242 [==============================] - 0s 122us/step - loss: 0.3693 - acc: 0.8347
Epoch 260/300
242/242 [==============================] - 0s 124us/step - loss: 0.3683 - acc: 0.8388
Epoch 261/300
242/242 [==============================] - 0s 123us/step - loss: 0.3690 - acc: 0.8430
Epoch 262/300
242/242 [==============================] - 0s 125us/step - loss: 0.3652 - acc: 0.8388
Epoch 263/300
242/242 [==============================] - 0s 126us/step - loss: 0.3677 - acc: 0.8347
Epoch 264/300
242/242 [==============================] - 0s 125us/step - loss: 0.3677 - acc: 0.8347
Epoch 265/300
242/242 [==============================] - 0s 125us/step - loss: 0.3814 - acc: 0.8430
Epoch 266/300
242/242 [==============================] - 0s 122us/step - loss: 0.3878 - acc: 0.8347
Epoch 267/300
242/242 [==============================] - 0s 122us/step - loss: 0.3708 - acc: 0.8347
Epoch 268/300
242/242 [==============================] - 0s 130us/step - loss: 0.3663 - acc: 0.8306
Epoch 269/300
242/242 [==============================] - 0s 125us/step - loss: 0.3741 - acc: 0.8264
Epoch 270/300
242/242 [==============================] - 0s 124us/step - loss: 0.3709 - acc: 0.8347
Epoch 271/300
242/242 [==============================] - 0s 124us/step - loss: 0.3628 - acc: 0.8430
Epoch 272/300
242/242 [==============================] - 0s 139us/step - loss: 0.3671 - acc: 0.8471
Epoch 273/300
242/242 [==============================] - 0s 129us/step - loss: 0.3640 - acc: 0.8430
Epoch 274/300
242/242 [==============================] - 0s 122us/step - loss: 0.3669 - acc: 0.8306
Epoch 275/300
242/242 [==============================] - 0s 127us/step - loss: 0.3725 - acc: 0.8306
Epoch 276/300
242/242 [==============================] - 0s 131us/step - loss: 0.3747 - acc: 0.8388
Epoch 277/300
242/242 [==============================] - 0s 127us/step - loss: 0.3638 - acc: 0.8430
Epoch 278/300
242/242 [==============================] - 0s 130us/step - loss: 0.4032 - acc: 0.8306
Epoch 279/300
242/242 [==============================] - 0s 127us/step - loss: 0.3787 - acc: 0.8388
Epoch 280/300
242/242 [==============================] - 0s 121us/step - loss: 0.3636 - acc: 0.8512
Epoch 281/300
242/242 [==============================] - 0s 130us/step - loss: 0.3599 - acc: 0.8554
Epoch 282/300
242/242 [==============================] - 0s 123us/step - loss: 0.3689 - acc: 0.8347
Epoch 283/300
242/242 [==============================] - 0s 128us/step - loss: 0.3785 - acc: 0.8306
Epoch 284/300
242/242 [==============================] - 0s 126us/step - loss: 0.3619 - acc: 0.8430
Epoch 285/300
242/242 [==============================] - 0s 125us/step - loss: 0.3663 - acc: 0.8471
Epoch 286/300
242/242 [==============================] - 0s 127us/step - loss: 0.3636 - acc: 0.8306
Epoch 287/300
242/242 [==============================] - 0s 126us/step - loss: 0.3804 - acc: 0.8306
Epoch 288/300
242/242 [==============================] - 0s 130us/step - loss: 0.3760 - acc: 0.8471
Epoch 289/300
242/242 [==============================] - 0s 125us/step - loss: 0.3727 - acc: 0.8388
Epoch 290/300
242/242 [==============================] - 0s 124us/step - loss: 0.3601 - acc: 0.8471
Epoch 291/300
242/242 [==============================] - 0s 125us/step - loss: 0.3667 - acc: 0.8347
Epoch 292/300
242/242 [==============================] - 0s 124us/step - loss: 0.3678 - acc: 0.8388
Epoch 293/300
242/242 [==============================] - 0s 128us/step - loss: 0.3625 - acc: 0.8595
Epoch 294/300
242/242 [==============================] - 0s 125us/step - loss: 0.3794 - acc: 0.8388
Epoch 295/300
242/242 [==============================] - 0s 132us/step - loss: 0.3635 - acc: 0.8388
Epoch 296/300
242/242 [==============================] - 0s 125us/step - loss: 0.3685 - acc: 0.8430
Epoch 297/300
242/242 [==============================] - 0s 125us/step - loss: 0.3695 - acc: 0.8388
Epoch 298/300
242/242 [==============================] - 0s 121us/step - loss: 0.3844 - acc: 0.8347
Epoch 299/300
242/242 [==============================] - 0s 130us/step - loss: 0.3653 - acc: 0.8471
Epoch 300/300
242/242 [==============================] - 0s 142us/step - loss: 0.3721 - acc: 0.8471

Out[67]:

<keras.callbacks.History at 0x7f74eb3f4da0>

In [68]:

Y_pred_nn = model.predict(X_test)

In [69]:

Y_pred_nn.shape

Out[69]:

(61, 1)

In [70]:

rounded = [round(x[0]) for x in Y_pred_nn]

Y_pred_nn = rounded

In [71]:

score_nn = round(accuracy_score(Y_pred_nn,Y_test)*100,2)

print("The accuracy score achieved using Neural Network is: "+str(score_nn)+" %")

#Note: Accuracy of 85% can be achieved on the test set, by setting epochs=2000, and number of nodes = 11.

The accuracy score achieved using Neural Network is: 80.33 %

VI. Output final score
In [72]:

scores = [score_lr,score_nb,score_svm,score_knn,score_dt,score_rf,score_xgb,score_nn]
algorithms = ["Logistic Regression","Naive Bayes","Support Vector Machine","K-Nearest Neighbors","Decision Tree","Random Forest","XGBoost","Neural Network"]    

for i in range(len(algorithms)):
    print("The accuracy score achieved using "+algorithms[i]+" is: "+str(scores[i])+" %")

The accuracy score achieved using Logistic Regression is: 85.25 %
The accuracy score achieved using Naive Bayes is: 85.25 %
The accuracy score achieved using Support Vector Machine is: 81.97 %
The accuracy score achieved using K-Nearest Neighbors is: 67.21 %
The accuracy score achieved using Decision Tree is: 81.97 %
The accuracy score achieved using Random Forest is: 95.08 %
The accuracy score achieved using XGBoost is: 85.25 %
The accuracy score achieved using Neural Network is: 80.33 %

In [73]:

sns.set(rc={'figure.figsize':(15,8)})
plt.xlabel("Algorithms")
plt.ylabel("Accuracy score")

sns.barplot(algorithms,scores)

Out[73]:

<matplotlib.axes._subplots.AxesSubplot at 0x7f74ea800eb8>

Hey arbaaz there random forest has good result as compare to other algorithms

